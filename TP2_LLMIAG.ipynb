{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLx29xWNXyY2"
      },
      "outputs": [],
      "source": [
        "!pip install pinecone-client transformers flask requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "A8hHlrx0kFHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install odfpy"
      ],
      "metadata": {
        "id": "iQWt5wfIvR_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p docs_odt  # Si no existe la carpeta lo creamos\n",
        "!mv isaias_cv.odt  octavio_cv.odt santiago_cv.odt docs_odt/"
      ],
      "metadata": {
        "id": "i2jD_jAVyhG8"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "import re\n",
        "import os\n",
        "import odf.opendocument\n",
        "import odf.text\n",
        "import odf.element\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "from pinecone import Pinecone\n",
        "from pinecone import ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "JtZ2RFsQkKit"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE = {\n",
        "    \"API_KEY\": \"pcsk_2b78HU_TqDQRdqmBRhgvc6RFtLamaqMW6NTqq7kziXPLcZEa4aabk5iTSwraqVCx3dF6iy\",\n",
        "    \"ENVIRONMENT\": \"us-west1-gcp\"\n",
        "}"
      ],
      "metadata": {
        "id": "THc6yMrZkRiI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Pinecone by creating an instance of the Pinecone class\n",
        "pinecone = Pinecone(api_key=PINECONE[\"API_KEY\"], environment=PINECONE[\"ENVIRONMENT\"]) # Changed this line\n",
        "\n",
        "index_name = \"rag-index\""
      ],
      "metadata": {
        "id": "UaVuZM_IkUWZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = pinecone.Index(index_name)\n",
        "# Modelo para embeddings y LLM\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2AoBMwozM38",
        "outputId": "2884b336-d707-45d6-9abe-cd440acb844f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para leer y extraer texto desde archivos .odt\n",
        "def leer_documentos_odt(ruta_archivo):\n",
        "    documento = odf.opendocument.load(ruta_archivo)\n",
        "    contenido = []\n",
        "    for elemento in documento.getElementsByType(odf.text.P):\n",
        "        # Extraer el texto de cada elemento <P>\n",
        "        texto_parrafo = ''.join([node.data for node in elemento.childNodes if isinstance(node, odf.element.Text)])\n",
        "        if texto_parrafo.strip():  # Asegurarse de que el texto no esté vacío\n",
        "            contenido.append(texto_parrafo.strip())\n",
        "    return \"\\n\".join(contenido)\n",
        "\n",
        "# Directorio que contiene los archivos ODT\n",
        "ruta_directorio = \"/content/docs_odt\"  # Cambia esta ruta según tu caso\n",
        "\n",
        "# Subir los documentos al índice de Pinecone\n",
        "for nombre_archivo in os.listdir(ruta_directorio):\n",
        "    if nombre_archivo.endswith(\".odt\"):\n",
        "        ruta_archivo = os.path.join(ruta_directorio, nombre_archivo)\n",
        "        try:\n",
        "            # Leer el contenido del archivo\n",
        "            contenido_documento = leer_documentos_odt(ruta_archivo)\n",
        "\n",
        "            # Generar el embedding\n",
        "            vector = embedder.encode(contenido_documento).tolist()\n",
        "\n",
        "            # Definir un namespace basado en el nombre del archivo (sin extensión)\n",
        "            namespace = os.path.splitext(nombre_archivo)[0].lower()\n",
        "\n",
        "            # Subir el vector al índice con metadatos\n",
        "            metadata = {\"text\": contenido_documento, \"filename\": nombre_archivo}\n",
        "            index.upsert([(namespace, vector, metadata)], namespace=namespace)\n",
        "            print(f\"Archivo {nombre_archivo} subido correctamente al namespace '{namespace}'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al procesar {nombre_archivo}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw1ei01dykno",
        "outputId": "3ec72076-0769-4029-9a46-97f091e626d9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo isaias_cv.odt subido correctamente al namespace 'isaias_cv'.\n",
            "Archivo santiago_cv.odt subido correctamente al namespace 'santiago_cv'.\n",
            "Archivo octavio_cv.odt subido correctamente al namespace 'octavio_cv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar el estado del índice\n",
        "stats = index.describe_index_stats()\n",
        "print(stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYJ3j-Z6z4vg",
        "outputId": "7b899720-7da7-409b-9114-ae19c7945244"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dimension': 384,\n",
            " 'index_fullness': 0.0,\n",
            " 'namespaces': {'isaias_cv': {'vector_count': 1},\n",
            "                'octavio_cv': {'vector_count': 1},\n",
            "                'santiago_cv': {'vector_count': 1}},\n",
            " 'total_vector_count': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# El índice ya existe en Pinecone , dado que fue creado para el trabajo anterior."
      ],
      "metadata": {
        "id": "5Bv-h8GHmfLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = pipeline(\"text-generation\", model=\"gpt2\", max_length=100)\n",
        "#llm = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")"
      ],
      "metadata": {
        "id": "zOOw_Ncqm0Vo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decidir el agente basado en el prompt\n",
        "def decide_agent(prompt):\n",
        "    if re.search(r'isaias', prompt, re.IGNORECASE):\n",
        "        return \"isaias_cv\"\n",
        "    elif re.search(r'octavio', prompt, re.IGNORECASE):\n",
        "        return \"octavio_cv\"\n",
        "    elif re.search(r'santiago', prompt, re.IGNORECASE):\n",
        "        return \"santiago_cv\"\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "TIpM3USInNsb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_context(prompt, namespace):\n",
        "    # Connect to the Pinecone index\n",
        "    index = pinecone.Index(index_name)\n",
        "    vector = embedder.encode(prompt).tolist()\n",
        "    # Change this line to use keyword arguments for 'vector'\n",
        "    result = index.query(vector=vector, top_k=1, namespace=namespace)\n",
        "    if result[\"matches\"]:\n",
        "        # Access metadata directly as a dictionary\n",
        "        return result[\"matches\"][0].metadata['text']\n",
        "    else:\n",
        "        # Return a message indicating no context was found to avoid the error\n",
        "        return \"No se encontró información relevante para la consulta.\""
      ],
      "metadata": {
        "id": "AAng5ebhnU6c"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar respuesta usando el LLM\n",
        "#def generate_response(prompt, context):\n",
        "#    full_prompt = f\"Contexto: {context}\\nPregunta: {prompt}\\nRespuesta:\"\n",
        "#    response = llm(full_prompt)\n",
        "#    return response[0]['generated_text']\n",
        "def generate_response(prompt, context):\n",
        "    full_prompt = f\"Contexto: {context}\\nPregunta: {prompt}\\nRespuesta:\"\n",
        "    response = llm(full_prompt, max_new_tokens=50)[0]  # Get the first element\n",
        "    return response['generated_text']  # Access the generated text\n",
        "# Sistema principal\n",
        "def multiagent_system(prompt):\n",
        "    agent = decide_agent(prompt)\n",
        "    if agent:\n",
        "        context = retrieve_context(prompt, namespace=agent)\n",
        "        return generate_response(prompt, context)\n",
        "    else:\n",
        "        return \"No se pudo determinar el agente para la consulta.\""
      ],
      "metadata": {
        "id": "Wz3rlasyncd6"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consulta ejemplo al namespace 'isaias'\n",
        "query_text = \"¿Qué experiencia tiene Isaias?\"\n",
        "query_vector = embedder.encode(query_text).tolist()\n",
        "\n",
        "result = index.query(vector=query_vector, top_k=1, namespace=\"isaias_cv\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "272llFTJnf4c",
        "outputId": "08f25bd3-ee23-4663-e3f7-b8147fd06b37"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'matches': [{'id': 'isaias_cv', 'score': 0.165477365, 'values': []}],\n",
            " 'namespace': 'isaias_cv',\n",
            " 'usage': {'read_units': 5}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  prompt = input(\"Ingresa tu consulta (o escribe 'salir' para terminar): \")\n",
        "  if prompt.lower() == \"salir\":\n",
        "    break\n",
        "  response = multiagent_system(prompt)\n",
        "  print(response)"
      ],
      "metadata": {
        "id": "W2UoVjBJ---l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}